# DiaHalu
This is the repository for the paper **DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models**.
DiaHalu is a dialogue-level hallucination benchmark, consisting of both factuality and faithfulness hallucination, naturally generated by two ChatGPT3.5s or two GPT4s.
This benchmark covers four dialogue domains and five hallucination subtypes, extending from factuality and faithfulness hallucination.

[[ARXIV]](https://arxiv.org/abs/2403.00896)

# Versions
The 1st version (corresponding to arxiv v1) can be referred to as Diahalu_V1.xlsx.

The 2nd version (corresponding to arxiv v2, coming soon) of DiaHalu can be referred to as DiaHalu_V2.xlsx. In this version, we augment the dataset from 748 to 1103 samples. 
GPT4 generates the latest 355 samples.

# Citation
@article{DBLP:journals/corr/abs-2403-00891,
  author       = {Kedi Chen and
                  Jie Zhou and
                  Qin Chen and
                  Shunyu Liu and
                  Liang He},
  title        = {A Regularization-based Transfer Learning Method for Information Extraction
                  via Instructed Graph Decoder},
  journal      = {CoRR},
  volume       = {abs/2403.00891},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.00891},
  doi          = {10.48550/ARXIV.2403.00891},
  eprinttype    = {arXiv},
  eprint       = {2403.00891},
  timestamp    = {Thu, 13 Jun 2024 21:48:06 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-00891.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
